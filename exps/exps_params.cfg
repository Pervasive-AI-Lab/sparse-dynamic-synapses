#!/usr/bin/env python
# -*- coding: utf-8 -*-

################################################################################
# Copyright (c) 2019. Vincenzo Lomonaco. All rights reserved.                  #
# Copyrights licensed under the CC BY 4.0 License.                             #
# See the accompanying LICENSE file for terms.                                 #
#                                                                              #
# Date: 8-11-2019                                                              #
# Author: Vincenzo Lomonaco                                                    #
# E-mail: vincenzo.lomonaco@unibo.it                                           #
# Website: vincenzolomonaco.com                                                #
################################################################################

### Parameters for the Sparse Dynamic Synapses project ###
# For more details about the parameters meaning check out the README.md file.

[DEFAULT]

# benchmark
benchmark = cifar
mnist_mode = perm
num_batch = 10
cumul = False

# sparsification
sparsify = True
weight_sparsity_fc = 0.1
weight_sparsity_conv = 0.99
percent_on_fc = 0.1
percent_on_conv = 0.99
boost_strength = 0
k_inference_factor = 1
boost_strength_factor = 1
duty_cycle_period = 1000

# model
cnn = True
hidden_units = 1024
hidden_layers = 1
dropout = 0.5

# optimizer
lr = 0.01
nesterov = False
momentum = 0
weight_decay = 0
mb_size = 128
train_ep = 30
train_ep_inc = 30

# others
record_stats = True

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;
; These are the parameters for the experiments of the project

[CNNCumul]

# benchmark
benchmark = cifar
mnist_mode = perm
num_batch = 10
cumul = True

# sparsification
sparsify = False
percent_on = 0.10
k_inference_factor = 1
boost_strength = 0
boost_strength_factor = 1
duty_cycle_period = 1000
weight_sparsity = 0.4

# model
cnn = True
hidden_units = 1024
hidden_layers = 2
dropout = 0.5

# optimizer
lr = 0.001
nesterov = False
momentum = 0
weight_decay = 0
mb_size = 128
train_ep = 60
train_ep_inc = 60

[Cifar100MLP]

# benchmark
benchmark = cifar
mnist_mode = perm
num_batch = 10
cumul = False

# sparsification
sparsify = True
percent_on = 0.10
k_inference_factor = 1
boost_strength = 0
boost_strength_factor = 1
duty_cycle_period = 1000
weight_sparsity = 0.4

# model
cnn = False
hidden_units = 4000
hidden_layers = 2
dropout = 0.5

# optimizer
lr = 0.0001
nesterov = False
momentum = 0
weight_decay = 0
mb_size = 128
train_ep = 60
train_ep_inc = 60

[Cifar100Cumul]

# benchmark
benchmark = cifar
mnist_mode = perm
num_batch = 10
cumul = True

# sparsification
sparsify = False
percent_on = 0.10
k_inference_factor = 1
boost_strength = 0
boost_strength_factor = 1
duty_cycle_period = 1000
weight_sparsity = 0.4

# model
cnn = False
hidden_units = 4000
hidden_layers = 2
dropout = 0.5

# optimizer
lr = 0.0001
nesterov = False
momentum = 0
weight_decay = 0
mb_size = 128
train_ep = 60
train_ep_inc = 60

[SplitMNIST]

# benchmark
mnist_mode = split
num_batch = 5

# sparsification
sparsify = False
percent_on = 0.008
k_inference_factor = 1
boost_strength = 0
boost_strength_factor = 1
duty_cycle_period = 1000
weight_sparsity = 0.5

# model
cnn = False
num_classes = 10
hidden_units = 4000
hidden_layers = 2
dropout = 0.5

# optimizer
lr = 0.1
nesterov = False
momentum = 0.9
weight_decay = 0
mb_size = 128
train_ep = 30
train_ep_inc = 30

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;
; These are the parameters for the *OLD* experiments of the project
; To be sure to reproduce the results of those a commit rollback may
; be needed.

[MLP_1Layer]

# this gets 64% vs 85% (sparse)
# 2,6% naturally sparse vs 0,05% sparse

# sparsification
sparsify = False
percent_on = 0.001
k_inference_factor = 1
boost_strength = 1.0
boost_strength_factor = 1
duty_cycle_period = 1000

# model
cnn = False
num_classes = 10
hidden_units = 4000
hidden_layers = 1
dropout = 0.5

# optimizer
lr = 0.1
nesterov = True
momentum = 0.9
weight_decay = 1e-4
mb_size = 128
train_ep = 2
train_ep_inc = 2

[MLP_1LayerSparse]

# this gets 64% vs 85% (sparse)
# 2,6% naturally sparse vs 0,05% sparse

# sparsification
sparsify = True
percent_on = 0.001
k_inference_factor = 1
boost_strength = 1.0
boost_strength_factor = 1
duty_cycle_period = 1000

# model
cnn = False
num_classes = 10
hidden_units = 4000
hidden_layers = 1
dropout = 0.5

# optimizer
lr = 0.1
nesterov = True
momentum = 0.9
weight_decay = 1e-4
mb_size = 128
train_ep = 2
train_ep_inc = 2

[MLP_2Layers]

# this gets 47% vs 60% (sparse)
# 13% naturally sparse vs 0,05% sparse

# sparsification
sparsify = False
percent_on = 0.001
k_inference_factor = 1
boost_strength = 1.0
boost_strength_factor = 1
duty_cycle_period = 1000

# model
cnn = False
num_classes = 10
hidden_units = 4000
hidden_layers = 2
dropout = 0.5

# optimizer
lr = 0.1
nesterov = True
momentum = 0.9
weight_decay = 1e-4
mb_size = 128
train_ep = 2
train_ep_inc = 2

[MLP_2LayersSparse]

# this gets 47% vs 60% (sparse)
# 13% naturally sparse vs 0,05% sparse

# sparsification
sparsify = True
percent_on = 0.001
k_inference_factor = 1
boost_strength = 1.0
boost_strength_factor = 1
duty_cycle_period = 1000

# model
cnn = False
num_classes = 10
hidden_units = 4000
hidden_layers = 2
dropout = 0.5

# optimizer
lr = 0.1
nesterov = True
momentum = 0.9
weight_decay = 1e-4
mb_size = 128
train_ep = 2
train_ep_inc = 2

[CNN]

# this gets 53% vs 57% (sparse)
# 2,7% naturally sparse vs 0,7% sparse

sparsify = False
percent_on = 0.05
k_inference_factor = 1
boost_strength = 1.0
boost_strength_factor = 1
duty_cycle_period = 1000

# model
cnn = True
num_classes = 10
hidden_layers = 2
hidden_units = 512
dropout = 0.5

# optimizer
lr = 0.1
nesterov = True
momentum = 0.9
weight_decay = 1e-4
mb_size = 128
train_ep = 2
train_ep_inc = 2

[CNNSparse]

# this gets 53% vs 57% (sparse)
# 2,7% naturally sparse vs 0,7% sparse

sparsify = True
percent_on = 0.05
k_inference_factor = 1
boost_strength = 1.0
boost_strength_factor = 1
duty_cycle_period = 1000

# model
cnn = True
num_classes = 10
hidden_layers = 2
hidden_units = 512
dropout = 0.5

# optimizer
lr = 0.1
nesterov = True
momentum = 0.9
weight_decay = 1e-4
mb_size = 128
train_ep = 2
train_ep_inc = 2
#!/usr/bin/env python
# -*- coding: utf-8 -*-

################################################################################
# Copyright (c) 2019. Vincenzo Lomonaco. All rights reserved.                  #
# Copyrights licensed under the CC BY 4.0 License.                             #
# See the accompanying LICENSE file for terms.                                 #
#                                                                              #
# Date: 8-11-2019                                                              #
# Author: Vincenzo Lomonaco                                                    #
# E-mail: vincenzo.lomonaco@unibo.it                                           #
# Website: vincenzolomonaco.com                                                #
################################################################################

#
# Parameters for the Sparse Dynamic Synapses project
#

[DEFAULT]

# sparsification
sparsify = False
percent_on = 0.05
k_inference_factor = 1
boost_strength = 1.0
boost_strength_factor = 1
duty_cycle_period = 1000

# model
cnn = True
num_classes = 10
hidden_layers = 2
hidden_units = 512
dropout = 0.5

# optimizer
lr = 0.1
nesterov = True
momentum = 0.9
weight_decay = 1e-4
mb_size = 128
train_ep = 2

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;
; These are the parameters for the experiments of the project


[MLP_1Layer]

# this gets 64% vs 85% (sparse)
# 2,6% naturally sparse vs 0,05% sparse

# sparsification
sparsify = False
percent_on = 0.001
k_inference_factor = 1
boost_strength = 1.0
boost_strength_factor = 1
duty_cycle_period = 1000

# model
cnn = False
num_classes = 10
hidden_units = 4000
hidden_layers = 1
dropout = 0.5

# optimizer
lr = 0.1
nesterov = True
momentum = 0.9
weight_decay = 1e-4
mb_size = 128
train_ep = 2

[MLP_2Layers]

# this gets 47%% vs 60% (sparse)
# 13% naturally sparse vs 0,05% sparse

# sparsification
sparsify = False
percent_on = 0.001
k_inference_factor = 1
boost_strength = 1.0
boost_strength_factor = 1
duty_cycle_period = 1000

# model
cnn = False
num_classes = 10
hidden_units = 4000
hidden_layers = 2
dropout = 0.5

# optimizer
lr = 0.1
nesterov = True
momentum = 0.9
weight_decay = 1e-4
mb_size = 128
train_ep = 2


[CNN]

# this gets 53% vs 57% (sparse)
# 2,7% naturally sparse vs 0,7% sparse

sparsify = True
percent_on = 0.05
k_inference_factor = 1
boost_strength = 1.0
boost_strength_factor = 1
duty_cycle_period = 1000

# model
cnn = True
num_classes = 10
hidden_layers = 2
hidden_units = 512
dropout = 0.5

# optimizer
lr = 0.1
nesterov = True
momentum = 0.9
weight_decay = 1e-4
mb_size = 128
train_ep = 2